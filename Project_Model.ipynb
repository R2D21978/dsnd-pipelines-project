{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and install English language model for spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import all libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('starter/data/reviews.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DF stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separate features from labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = df.drop('Recommended IND' , axis= 1)\n",
    "Y = df['Recommended IND'].copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( x, y, test_size=0.2, random_state=27, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into numerical, categorical, and text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns that are numeric \n",
    "num_features = X.select_dtypes(exclude=['object']).columns\n",
    "print('Numerical features:', num_features)\n",
    "\n",
    "# Select columns that are categorical \n",
    "cat_features = X.select_dtypes(include=['object']).columns\n",
    "print('Categorical features:', cat_features)\n",
    "\n",
    "# Select column with review text\n",
    "text_features = X[['Review Text']].columns\n",
    "print('Review Text features:', text_features)\n",
    "\n",
    "# Show first rows of the dataset to check\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline for num features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline = Pipeline([\n",
    "    # fill missing values with most frequent value\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  \n",
    "    \n",
    "    # scale numbers to range 0-1\n",
    "    ('scaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "num_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline for cat features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline for cat features\n",
    "cat_pipeline = Pipeline([     \n",
    "    # fill missing values with most frequent value\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    \n",
    "    # convert cat to num (each unique category -> a number)\n",
    "    ('ordinal_encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)),\n",
    "    \n",
    "    # create one-hot columns (0/1) for each category\n",
    "    ('cat_encoder', OneHotEncoder(sparse_output=False, handle_unknown='ignore')),\n",
    "])\n",
    "\n",
    "cat_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text features, Count Characters Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CountCharacter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, character: str):\n",
    "        # Character we want to count\n",
    "        self.character = character\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Just return self\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Count how many times the character appears in each text\n",
    "        # (text or \"\") -> handle None values as empty string\n",
    "        return [[(text or \"\").count(self.character)] for text in X]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess text to make it 1D for transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_text_preprocess = Pipeline([\n",
    "    (\n",
    "        'dimension_reshaper',\n",
    "        FunctionTransformer(\n",
    "            np.reshape,          # reshape array\n",
    "            kw_args={'newshape':-1},  # make 1D\n",
    "        ),\n",
    "    ),\n",
    "])\n",
    "\n",
    "# Create features by counting specific characters\n",
    "feature_engineering = FeatureUnion([\n",
    "    ('count_spaces', CountCharacter(character=' ')),      # count spaces\n",
    "    ('count_exclamations', CountCharacter(character='!')), # count exclamation marks\n",
    "    ('count_question_marks', CountCharacter(character='?')), # count question marks\n",
    "])\n",
    "\n",
    "# Combine preprocessing and feature engineering into one pipeline\n",
    "character_counts_pipeline = Pipeline([\n",
    "    ('initial_text_preprocess', initial_text_preprocess), # reshape text\n",
    "    ('feature_engineering', feature_engineering),         # count characters\n",
    "])\n",
    "\n",
    "character_counts_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# custom transformer for lemmatizing text and removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpacyLemmatizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, nlp):\n",
    "        # Store the spaCy model (nlp) to use for processing text\n",
    "        self.nlp = nlp\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # just return self\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Process each text in X\n",
    "        lemmatized = [\n",
    "            ' '.join(\n",
    "                token.lemma_ for token in doc  # take the base form of each word\n",
    "                if not token.is_stop           # skip stopwords like \"the\", \"and\", \"is\"\n",
    "            )\n",
    "            for doc in self.nlp.pipe(X)        # use spaCy to process all texts \n",
    "        ]\n",
    "        return lemmatized                     # return list of cleaned, lemmatized texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_pipeline = Pipeline([\n",
    "    # reshape input to 1D array \n",
    "    (\n",
    "        'dimension_reshaper',\n",
    "        FunctionTransformer(\n",
    "            np.reshape,\n",
    "            kw_args={'newshape':-1},\n",
    "        ),\n",
    "    ),\n",
    "    # lemmatize text and remove stopwords\n",
    "    (\n",
    "        'lemmatizer',\n",
    "        SpacyLemmatizer(nlp=nlp),\n",
    "    ),\n",
    "    # convert text into TF-IDF numeric features\n",
    "    (\n",
    "        'tfidf_vectorizer',\n",
    "        TfidfVectorizer(\n",
    "            stop_words='english',  # remove common English words\n",
    "        ),\n",
    "    ),\n",
    "])\n",
    "\n",
    "tfidf_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine all feature processing steps into one transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_engineering = ColumnTransformer([\n",
    "    # process num features with num_pipeline\n",
    "    ('num', num_pipeline, num_features),\n",
    "    \n",
    "    # process cat features with cat_pipeline\n",
    "    ('cat', cat_pipeline, cat_features),\n",
    "    \n",
    "    # count special characters in text (spaces, !, ?)\n",
    "    ('character_counts', character_counts_pipeline, text_features),\n",
    "    \n",
    "    # process text into TF-IDF features\n",
    "    ('tfidf_text', tfidf_pipeline, text_features),\n",
    "])\n",
    "\n",
    "feature_engineering"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
