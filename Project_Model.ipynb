{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and install English language model for spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import all libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('starter/data/reviews.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DF stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separate features from labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Recommended IND' , axis= 1)\n",
    "Y = df['Recommended IND'].copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, Y, test_size=0.2, random_state=27, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into numerical, categorical, and text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns that are numeric \n",
    "num_features = X.select_dtypes(exclude=['object']).columns\n",
    "print('Numerical features:', num_features)\n",
    "\n",
    "# Select columns that are categorical \n",
    "cat_features = X.select_dtypes(include=['object']).columns\n",
    "print('Categorical features:', cat_features)\n",
    "\n",
    "# Select column with review text\n",
    "text_features = X[['Review Text']].columns\n",
    "print('Review Text features:', text_features)\n",
    "\n",
    "# Show first rows of the dataset to check\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline for num features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline = Pipeline([\n",
    "    # fill missing values with most frequent value\n",
    "    ('imputer', SimpleImputer(strategy='median')),  \n",
    "    \n",
    "    # scale numbers to range 0-1\n",
    "    ('scaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "num_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline for cat features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline for cat features\n",
    "cat_pipeline = Pipeline([     \n",
    "    # fill missing values with most frequent value\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    \n",
    "    # create one-hot columns (0/1) for each category\n",
    "    ('cat_encoder', OneHotEncoder(sparse_output=False, handle_unknown='ignore')),\n",
    "])\n",
    "\n",
    "cat_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text features, Count Characters Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CountCharacter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, character: str):\n",
    "        # Character we want to count\n",
    "        self.character = character\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Just return self\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Count how many times the character appears in each text\n",
    "        # (text or \"\") -> handle None values as empty string\n",
    "        return [[(text or \"\").count(self.character)] for text in X]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess text to make it 1D for transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_text_preprocess = Pipeline([('dimension_reshaper', FunctionTransformer(lambda x: x.values.ravel()))])\n",
    "\n",
    "# Create features by counting specific characters\n",
    "feature_engineering = FeatureUnion([\n",
    "    ('count_spaces', CountCharacter(character=' ')),      # count spaces\n",
    "    ('count_exclamations', CountCharacter(character='!')), # count exclamation marks\n",
    "    ('count_question_marks', CountCharacter(character='?')), # count question marks\n",
    "])\n",
    "\n",
    "# Combine preprocessing and feature engineering into one pipeline\n",
    "character_counts_pipeline = Pipeline([\n",
    "    ('initial_text_preprocess', initial_text_preprocess), # reshape text\n",
    "    ('feature_engineering', feature_engineering),         # count characters\n",
    "])\n",
    "\n",
    "character_counts_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# custom transformer for lemmatizing text and removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpacyLemmatizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, nlp):\n",
    "        # Store the spaCy model (nlp) to use for processing text\n",
    "        self.nlp = nlp\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # just return self\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Process each text in X\n",
    "        lemmatized = [\n",
    "            ' '.join(\n",
    "                token.lemma_ for token in doc  # take the base form of each word\n",
    "                if not token.is_stop           # skip stopwords like \"the\", \"and\", \"is\"\n",
    "            )\n",
    "            for doc in self.nlp.pipe(X, batch_size=50)        # use spaCy to process all texts batch size - performance\n",
    "        ]\n",
    "        return lemmatized                     # return list of cleaned, lemmatized texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_pipeline = Pipeline([\n",
    "    # Reshape input data to a 1D array\n",
    "    ('dimension_reshaper', FunctionTransformer(np.reshape, kw_args={'newshape': -1})),\n",
    "    \n",
    "    # Lemmatize the text using spaCy\n",
    "    ('lemmatizer', SpacyLemmatizer(nlp=nlp)),\n",
    "    \n",
    "    # Convert processed text into a TF-IDF matrix\n",
    "    ('tfidf_vectorizer', TfidfVectorizer())  \n",
    "])\n",
    "tfidf_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine all feature processing steps into one transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_preprocessor = ColumnTransformer([\n",
    "    # Numeric pipeline: applies transformations to numerical features\n",
    "    ('num', num_pipeline, num_features),\n",
    "    \n",
    "    # Categorical pipeline: handles encoding/processing of categorical features\n",
    "    ('cat', cat_pipeline, cat_features),\n",
    "    \n",
    "    # Character counts pipeline: extracts custom features such as text length,\n",
    "    ('character_counts', character_counts_pipeline, text_features),\n",
    "    \n",
    "    # TF-IDF text pipeline: transforms text columns into TF-IDF vectors\n",
    "    ('tfidf_text', tfidf_pipeline, text_features),\n",
    "])\n",
    "feature_engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a complete machine learning pipeline\n",
    "model_pipeline = make_pipeline(\n",
    "    \n",
    "    # Preprocessing - applies all feature engineering transformations\n",
    "    full_preprocessor,\n",
    "    \n",
    "     # Model - a Random Forest classifier\n",
    "    RandomForestClassifier(random_state=27, class_weight='balanced')\n",
    ")\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "model_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set using the trained pipeline\n",
    "y_pred = model_pipeline.predict(X_test)\n",
    "\n",
    "# Print accuracy score (overall percentage of correct predictions)\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Print classification report (precision, recall, f1-score, support per class)\n",
    "print('\\nClassification Report:\\n', classification_report(y_test, y_pred))\n",
    "\n",
    "# Print confusion matrix (shows correct vs. incorrect predictions for each class)\n",
    "print('\\nConfusion Matrix:\\n', confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tune Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameter search space for RandomForestClassifier\n",
    "param_distributions = dict(\n",
    "    randomforestclassifier__max_features=['sqrt', 'log2'],\n",
    "    randomforestclassifier__n_estimators=[150, 200],\n",
    ")\n",
    "# Create a randomized search with cross-validation\n",
    "param_search = RandomizedSearchCV(\n",
    "    estimator=model_pipeline,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=6,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    "    verbose=3,\n",
    "    random_state=27\n",
    ")\n",
    "\n",
    "# Perform the randomized search on training data\n",
    "param_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print('Best Params:', param_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the best pipeline (with the best hyperparameters) from the search\n",
    "model_best = param_search.best_estimator_\n",
    "\n",
    "# Use the tuned model to make predictions on the test set\n",
    "y_pred_best = model_best.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print accuracy score of the tuned model\n",
    "print('Tuned Accuracy:', accuracy_score(y_test, y_pred_best))\n",
    "\n",
    "# Print classification report (precision, recall, f1-score for each class)\n",
    "print('\\nTuned Classification Report:\\n', classification_report(y_test, y_pred_best))\n",
    "\n",
    "# Print confusion matrix (comparison of true vs. predicted labels)\n",
    "print('\\nTuned Confusion Matrix:\\n', confusion_matrix(y_test, y_pred_best))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
